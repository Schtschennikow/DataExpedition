# HSE Career analysis

#### Libraries:
vk  
json  
time  
tqdm  
requests  
bs4  
datetime  
pandas  
re  
BeautifulSoup  
collections  
tqdm  
selenium  
argparse  

В ходе работы, были написаны три программы:

**vk_group_wall_scraper** — выгружает все посты со стены сообщества в ВК по его id. Также, пользователю небходим индивидуальный токен.
Запрашивает токен и иднтификационный номер ообщества. В результате работы создаёт json документ со текстами и временными метками всех постов со стены сообщества (в которых текст присутствует). В связи с ограничениями, накладываемыми ВК на работу апи, максимальное количество постов  которое можно выгрузить за сутки – 500000.

UPD В ходе работы над проектом, было решено не использовать данные из ВК.

**HSE_Career_website_scraper** — выгружает все посты из новостной ленты сайта Центра развития карьеры ВШЭ, вычленяет посты с дайджестами вакансий для студентов и выпускников, разбирает эти дайджесты на составляющие и сохраняет в виде CSV-таблиц и json'ов (в названиях сохраняется дата выгрузки данных).

**hh_companies_data_extractor** — выгружает компании с указанием их сферы деятельности с hh.ru. Сохраняет их в json и tsv.

Ход работы:

1. Сбор постов из новостной ленты сайта.
Проблема: данные о дате публикации новости оформлены в нестандартной манере.
Решение: написание кода, который обрабатывает значения указанные на сайте и преобразует их в iso формат

2. Парсинг страниц дайджестов.
Проблема: не смотря на видимость структуры вида "заголовок 1 уровня, заголовок 2 уровня, текст", весь тект представляет собой набор span'ов, а блоки текста зачастую разделены на несколько. (Исключение: название вакансий обладают специфическим стилем, который не менялся).
Решение: Использование ряда регулярныйх выражений.

3. Структурируем.

4. Вычленяем из текстов доступные данные.
